# -*- coding: utf-8 -*-
"""lab 13 neural_nets_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10vF_zXHB-QFprPQP5mtQGR7oeWO8GW1A

<img src="http://imgur.com/1ZcRyrc.png" style="float: left; margin: 20px; height: 55px">

# Intro to Regression with Tensorflow & Keras

_Author: Unknown_

------

**OBJECTIVES**

- Build regression models using `tensorflow` & `keras`
- Refine models by adjusting the architecture of a network
- Use regularization to attempt performance improvement
- Save and reuse the model
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l1, l2, l1_l2

cali = fetch_california_housing()
X, y = cali.data, cali.target

type(cali)

cali

"""### Part 1

Load the california housing data and create a dataframe called `cali_df` below.  Make sure to add the target feature and name this `price`. 
"""

cali_df = pd.DataFrame(cali.data,columns=cali.feature_names)
cali_df['price'] = pd.Series(cali.target)
cali_df.head()

cali_df.info()

cali_df.shape

"""### Part 2

Create a train/test split using some of the features in your X and setting y to be the `price` column.
"""

# Split into X and y
X = cali_df.drop('price', axis =1)
y= cali_df['price'].astype(int)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 42)

X_train.shape

"""### Part 3

Setup a `Sequential` model with one layer containing 24 nodes.  Make sure to include the output layer and use a `ReLU` activation for the hidden layer.
"""

#Create model topology
model = Sequential()

#input layer
model.add(Dense(8, activation  =  'relu', input_shape =(8,))) 
#first hidden layer
model.add(Dense(24, activation ='relu')) 

#output layer
model.add(Dense(1, activation  = None))



"""Set up the compilation of the network.  Use an `adam` optimizer and appropriate loss function with the mean squared error metric."""

# Compile
model.compile(loss = 'mean_squared_error', optimizer= 'adam')

"""### Part 4

Fit the model on the training data with 100 epochs (and sequester the output with `verbose = 0`). Save the fit model to the variable `history`.
"""

# Fit
history1 = model.fit(X_train, y_train,
             epochs = 100,
             batch_size = 512,
             validation_data =(X_test,y_test),
             verbose = 0)

"""### Part 5

Use `matplotlib` to plot the training loss and validation loss, and the training mean squared error alongside the validation data.  Side by side subplots please.
"""

# Visualize the loss
train_loss1 = history1.history['loss']
test_loss1 = history1.history['val_loss']

plt.figure(figsize=(12, 8))
plt.plot(train_loss1, label='Training loss', color='navy')
plt.plot(test_loss1, label='Testing loss', color='skyblue')
plt.legend();

#This is due to a classic mistake made by new practitioners, i.e. not normalizing their data before feeding them into a neural network (see the third point in this answer 
#for the same issue causing similar problems in a classification setting with a convolutional neural network).

"""### Part 6

Let's make a second network that is a bit deeper and more complex. Also, let's now use all the features and see if we help the model.  Use 3 layers, with 64, 128, and 64 nodes respectively in the hidden layers and a `ReLU` activation function.
"""

#Create model topology
model = Sequential()

#input layer
model.add(Dense(8, activation  = 'relu', input_shape =(8,)))  

#1st hidden layer
model.add(Dense(64, activation ='relu')) 
#2nd hidden layer
model.add(Dense(128, activation ='relu')) 
#3rd hidden layer
model.add(Dense(64, activation ='relu')) 

#output layer
model.add(Dense(1, activation  = None))

# Compile
model.compile(loss = 'mean_squared_error', optimizer= 'adam')

# Fit
history2 = model.fit(X_train, y_train,
             epochs = 100,
             batch_size = 512,
             validation_data =(X_test,y_test),
             verbose = 0)

# Visualize the loss
train_loss2 = history2.history['loss']
test_loss2 = history2.history['val_loss']

plt.figure(figsize=(12, 8))
plt.plot(train_loss2, label='Training loss', color='navy')
plt.plot(test_loss2, label='Testing loss', color='skyblue')
plt.legend();

"""### Part 7

Add a `BatchNormalization` layer prior to our first dense layer in the network above and repeat the fit.
"""

#Create model topology
model = Sequential()

#input layer
model.add(Dense(8, activation  = 'relu', input_shape =(8,)))  
model.add(BatchNormalization())

#1st hidden layer
model.add(Dense(64, activation ='relu')) 
#2nd hidden layer
model.add(Dense(128, activation ='relu')) 
#3rd hidden layer
model.add(Dense(64, activation ='relu')) 

#output layer
model.add(Dense(1, activation  = None))

# Compile
model.compile(loss = 'mean_squared_error', optimizer= 'adam')

# Fit
history3 = model.fit(X_train, y_train,
             epochs = 100,
             batch_size = 512,
             validation_data =(X_test,y_test),
             verbose = 0)

# Visualize the loss
train_loss3 = history3.history['loss']
test_loss3 = history3.history['val_loss']

plt.figure(figsize=(12, 8))
plt.plot(train_loss2, label='Training loss', color='navy')
plt.plot(test_loss2, label='Testing loss', color='skyblue')
plt.plot(train_loss3, label='Training loss BatchNorm', color='red')
plt.plot(test_loss3, label='Testing loss BatchNorm', color='orange')
plt.legend();

"""##### Does this change anything?"""

# BatchNormalization seems to produced smaller loss function at the early epochs and smoother loss fucntion overall

"""### Part 8

Early Stopping.  It seems that we may not need all 100 epochs to train the data.  Include an `EarlyStopping` callback in your model from above.  Set the `patience` equal to 5.  How many epochs do you think are appropriate?
"""

#according to the graph above, the loss graph is flatted at early stage, however, it might be because of the y-scale of the graph. We might want to ty epochs = 20 first

from tensorflow.keras.callbacks import EarlyStopping
es = EarlyStopping(patience=5)

#Create model topology
model = Sequential()

#input layer
model.add(Dense(8, activation  = 'relu', input_shape =(8,)))  
model.add(BatchNormalization())

#1st hidden layer
model.add(Dense(64, activation ='relu')) 
#2nd hidden layer
model.add(Dense(128, activation ='relu')) 
#3rd hidden layer
model.add(Dense(64, activation ='relu')) 

#output layer
model.add(Dense(1, activation  = None))

# Compile
model.compile(loss = 'mean_squared_error', optimizer= 'adam')

# Fit
history4 = model.fit(X_train, y_train,
             epochs = 20,
             batch_size = 512,
             validation_data =(X_test,y_test),
             callbacks =[es],
             verbose = 0)

# Visualize the loss
train_loss4 = history4.history['loss']
test_loss4 = history4.history['val_loss']

plt.figure(figsize=(12, 8))
plt.plot(train_loss4, label='Training loss', color='navy')
plt.plot(test_loss4, label='Testing loss', color='skyblue')

plt.legend();

"""### Part 9

Adding `Dropout`.  Let's add a 5% dropout to the second layer, and a 20% dropout to the third layer and see if we end up stopping sooner or performing better.
"""

#Create model topology
model = Sequential()

#input layer
model.add(Dense(8, activation  = 'relu', input_shape =(8,)))  
model.add(BatchNormalization())

#1st hidden layer
model.add(Dense(64, activation ='relu')) 
#2nd hidden layer
model.add(Dense(128, activation ='relu')) 
model.add(Dropout(0.05))
#3rd hidden layer
model.add(Dense(64, activation ='relu')) 
model.add(Dropout(0.2))

#output layer
model.add(Dense(1, activation  = None))

# Compile
model.compile(loss = 'mean_squared_error', optimizer= 'adam', metrics= ['mean_squared_error'])

# Fit
history5 = model.fit(X_train, y_train,
             epochs = 20,
             batch_size = 512,
             validation_data =(X_test,y_test),
             callbacks =[es],
             verbose = 0)

# Visualize the loss
train_loss5 = history5.history['loss']
test_loss5 = history5.history['val_loss']

plt.figure(figsize=(12, 8))
plt.plot(train_loss5, label='Training loss', color='navy')
plt.plot(test_loss5, label='Testing loss', color='skyblue')

plt.legend();

"""### Part 9 - continued: RMSE vs. Baseline

Compare the model aboves performance to that of the baseline model for the data.
"""

# Baselline model 
y_train.mean()

# Evaluate model on test data
score = model.evaluate(X_test,
                       y_test,
                       verbose=1)

labels = model.metrics_names

# Show model performance
print(f'{labels[0]}: {score[0]}')
print(f'{labels[1]}: {score[1]}')
print(f'root_mean_squared_error : {score[1]**(0.5)}')

# Visualize the MSE
train_MSE5 = history5.history['mean_squared_error']
test_MSE5 = history5.history['val_mean_squared_error']

plt.figure(figsize=(12, 8))
plt.plot(train_MSE5, label='Training MSE', color='navy')
plt.plot(test_MSE5, label='Testing MSE', color='skyblue')

plt.legend();

#Comparing to linear regression
from sklearn.linear_model import LinearRegression 
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score

#Instantiate the Regression model/Fit Model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Train Score
print(f'Train R-squared Score: {lr.score(X_train, y_train)}')
print(f'Test R-squared Score: {lr.score(X_test, y_test)}')

# Cross Val Score
print(f'Cross Val Score :{cross_val_score(lr, X_train, y_train, cv =5)}')
# Making prediction
y_preds = lr.predict(X_test)

#evaluate models with matrics
print(f'MSE is : {mean_squared_error(y_test, y_preds)}' )
print(f'RMSE is : {mean_squared_error(y_test, y_preds, squared = False)}' )

#Linear regression model  performed batter than neural net in terms of RMSE

"""### Part 10: Regularization and Scaling

Finally, we want to see if regularizing will improve the model.  Feed a model that is identical to the one above including dropout and include `l2` regularization in each of the dense layers of 0.01.  What is the RMSE of this model?  How does it compare to the baseline?
"""

# Scale data
sc = StandardScaler()
X_train_sc =sc.fit_transform(X_train)
X_test_sc =sc.transform(X_test)

#Create model topology
model = Sequential()

#input layer
model.add(Dense(8, activation  = 'relu', input_shape =(8,), kernel_regularizer = l2(0.01)))  
model.add(BatchNormalization())

#1st hidden layer
model.add(Dense(64, activation ='relu', kernel_regularizer = l2(0.01))) 
#2nd hidden layer
model.add(Dense(128, activation ='relu', kernel_regularizer = l2(0.01))) 
model.add(Dropout(0.05))
#3rd hidden layer
model.add(Dense(64, activation ='relu', kernel_regularizer = l2(0.01))) 
model.add(Dropout(0.2))

#output layer
model.add(Dense(1, activation  = None))

# Compile
model.compile(loss = 'mean_squared_error', optimizer= 'adam', metrics = ['mean_squared_error'])

# Fit
history6 = model.fit(X_train_sc, y_train,
             epochs = 20,
             batch_size = 512,
             validation_data =(X_test_sc,y_test),
             callbacks =[es],
             verbose = 1)

# Visualize the loss
train_loss6 = history6.history['loss']
test_loss6 = history6.history['val_loss']

plt.figure(figsize=(12, 8))
plt.plot(train_loss6, label='Training loss', color='navy')
plt.plot(test_loss6, label='Testing loss', color='skyblue')

plt.legend();

# Evaluate model on test data
score2 = model.evaluate(X_test_sc,
                       y_test,
                       verbose=1)

labels = model.metrics_names

# Show model performance
print(f'{labels[0]}: {score2[0]}')
print(f'{labels[1]}: {score2[1]}')
print(f'root_mean_squared_error : {score2[1]**(0.5)}')

# Visualize the MSE
train_MSE6 = history6.history['mean_squared_error']
test_MSE6 = history6.history['val_mean_squared_error']

plt.figure(figsize=(12, 8))
plt.plot(train_MSE6, label='Training MSE', color='navy')
plt.plot(test_MSE6, label='Testing MSE', color='skyblue')

plt.legend();

"""### Part 11: Saving the Model

Save the model as `cali_housing.h5`.  
"""

model.save("cali_housing.h5")
print("Saved model to disk")

